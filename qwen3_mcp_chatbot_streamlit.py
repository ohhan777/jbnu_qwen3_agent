import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
import torch
from typing import List, Tuple, Generator
import threading
from qwen_agent.agents import Assistant
from qwen_agent.tools.base import BaseTool, register_tool
from qwen_agent.utils.output_beautify import typewriter_print
import requests

llm_cfg = {
    'model': 'Qwen/Qwen3-8B',
    'model_server': 'http://localhost:8000/v1',  # vLLM API endpoint
    'api_key': 'EMPTY',
    'generate_cfg': {
        'top_p': 0.8,
        'temperature': 0.7,
        'max_tokens': 2048
    }
}


# Step 3: Create an agent with MCP tools for korea_weather
system_instruction = '''ë‹¹ì‹ ì€ í•œêµ­ ë‚ ì”¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 

ì‚¬ìš©ìê°€ ë‚ ì”¨ ì •ë³´ë¥¼ ìš”ì²­í•˜ë©´:
1. korea_weather MCP ì„œë²„ì˜ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤
2. ì¡°íšŒëœ ì •ë³´ë¥¼ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤
3. í•„ìš”í•œ ê²½ìš° ìœ„ë„/ê²½ë„ ì •ë³´ë¥¼ ìš”ì²­í•˜ê±°ë‚˜ ì„œìš¸(37.5665, 126.9780) ë“±ì˜ ì£¼ìš” ë„ì‹œ ì¢Œí‘œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤

ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì”¨ ë„êµ¬ë“¤:
- get_nowcast_observation: í˜„ì¬ ë‚ ì”¨ ê´€ì¸¡ ì •ë³´ (ê¸°ì˜¨, ê°•ìˆ˜ëŸ‰, ìŠµë„, í’ì†)
- get_nowcast_forecast: ì´ˆë‹¨ê¸° ì˜ˆë³´ (6ì‹œê°„ ì´ë‚´)
- get_short_term_forecast: ë‹¨ê¸° ì˜ˆë³´ (3ì¼ ì´ë‚´)

í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ê³ , ë‚ ì”¨ ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ë§Œì•½ ë‚ ì”¨ì™€ ê´€ê³„ ì—†ëŠ” ì§ˆë¬¸ì´ë¼ë©´ ë„êµ¬ ì‚¬ìš© ì—†ì´ ì¼ë°˜ì ì¸ ëŒ€í™”ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.

'''

# MCP ì„œë²„ ì„¤ì •
tools = [
    {
        "mcpServers": {
            "korea_weather": {
                "command": "uv",
                "args": [
                    "run",
                    "korea_weather.py"
                ]
            },
        }
    }
]

bot = Assistant(llm=llm_cfg,
                system_message=system_instruction,
                function_list=tools)

def test_vllm_connection():
    """vLLM ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        response = requests.get("http://localhost:8000/v1/models", timeout=5)
        if response.status_code == 200:
            print("âœ… vLLM ì„œë²„ ì—°ê²° ì„±ê³µ")
            models = response.json()
            for model in models['data']:
                print(f"   - ëª¨ë¸: {model['id']}")
                print(f"   - ìµœëŒ€ ê¸¸ì´: {model['max_model_len']}")
            return True
        else:
            print("âŒ vLLM ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜")
            return False
    except Exception as e:
        print(f"âŒ vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return False


def main():
    st.set_page_config(
        page_title="Qwen3-8B + vLLM + MCP Chatbot",
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    st.title("ğŸ¤– Qwen3-8B + vLLM + MCP Chatbot")
    st.markdown("Qwen3-8B ëª¨ë¸ê³¼ vLLM ì„œë²„, MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•œ AI ì±—ë´‡ì…ë‹ˆë‹¤.")
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Sidebar for controls
    with st.sidebar:
        st.header("ì„¤ì •")
        
        # ì„œë²„ ìƒíƒœ í™•ì¸
        st.subheader("ì„œë²„ ìƒíƒœ")
        if test_vllm_connection():
            st.success("âœ… vLLM ì„œë²„ ì—°ê²° ì„±ê³µ")
            st.info("""
            **vLLM ì„œë²„ ì •ë³´:**
            - ëª¨ë¸: Qwen/Qwen3-8B
            - ì—”ë“œí¬ì¸íŠ¸: http://localhost:8000
            - ìƒíƒœ: ì‹¤í–‰ ì¤‘
            """)
        else:
            st.error("âŒ vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨")
            st.info("""
            **vLLM ì„œë²„ ì‹œì‘ ë°©ë²•:**
            ```bash
            vllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1 --tensor-parallel-size 4
            ```
            """)
        
        # MCP ë„êµ¬ ì •ë³´
        st.subheader("MCP ë„êµ¬")
        st.info("""
        **ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:**
        - korea_weather: í•œêµ­ ë‚ ì”¨ ì •ë³´
        - ì‹¤ì‹œê°„ ë‚ ì”¨ ë°ì´í„° ì¡°íšŒ
        
        **ì‚¬ìš© ì˜ˆì‹œ:**
        - "ì„œìš¸ ë‚ ì”¨ ì•Œë ¤ì¤˜"
        - "ë¶€ì‚° ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”?"
        """)
        
        if st.button("ìƒˆ ì±„íŒ…", type="secondary"):
            st.session_state.messages = []
            st.rerun()
    
    # Display chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Chat input
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Display assistant response with streaming
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            # Stream the response
            response_plain_text = ""
            full_response = ""
            
            try:
                for response in bot.run(messages=st.session_state.messages):
                    # Streaming output
                    response_plain_text = typewriter_print(response, response_plain_text)
                    message_placeholder.markdown(response_plain_text + "â–Œ")
                    full_response = response_plain_text
                
                # Final response without cursor
                message_placeholder.markdown(full_response)
                
                # Append the bot responses to the chat history
                st.session_state.messages.extend(response)
                
            except Exception as e:
                error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                message_placeholder.markdown(error_msg)
                full_response = error_msg
            
            # Add assistant response to chat history
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()